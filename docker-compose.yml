networks:
  datakata:
    driver: bridge

volumes:
  postgres-data:
  clickhouse-data:
  kafka-data:
  minio-data:
  marquez-db-data:
  grafana-data:
  prometheus-data:
  spooldir-finished:
  spooldir-error:

services:

  # ─── PostgreSQL (Source DB) ────────────────────────────────────────
  postgres:
    image: postgres:16
    container_name: datakata-postgres
    environment:
      POSTGRES_DB: datakata
      POSTGRES_USER: datakata
      POSTGRES_PASSWORD: datakata
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./docker/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    command: >
      postgres
        -c wal_level=logical
        -c max_replication_slots=4
        -c max_wal_senders=4
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U datakata -d datakata"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - datakata

  # ─── MinIO (S3-compatible File Source) ─────────────────────────────
  minio:
    image: minio/minio:latest
    container_name: datakata-minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - datakata

  minio-init:
    image: minio/mc:latest
    container_name: datakata-minio-init
    depends_on:
      minio:
        condition: service_healthy
    volumes:
      - ./docker/minio/seed-data:/seed-data
    entrypoint: >
      /bin/sh -c "
        mc alias set local http://minio:9000 minioadmin minioadmin &&
        mc mb local/sales-data --ignore-existing &&
        mc cp /seed-data/sales-batch-001.csv local/sales-data/topics/sales.files/sales-batch-001.csv &&
        mc cp /seed-data/sales-batch-002.csv local/sales-data/topics/sales.files/sales-batch-002.csv &&
        echo 'MinIO seed complete'
      "
    networks:
      - datakata

  # ─── Apache Kafka (KRaft mode) ────────────────────────────────────
  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: datakata-kafka
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      CLUSTER_ID: "dGF0YS1rYXRhLWthZmthLWNsdXN0ZXI"
    volumes:
      - kafka-data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list > /dev/null 2>&1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - datakata

  # ─── Confluent Schema Registry ────────────────────────────────────
  schema-registry:
    image: confluentinc/cp-schema-registry:7.6.1
    container_name: datakata-schema-registry
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8085:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081/subjects || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks:
      - datakata

  # ─── Kafka Connect (Debezium + S3) ────────────────────────────────
  kafka-connect:
    build:
      context: ./docker/kafka-connect
      dockerfile: Dockerfile
    container_name: datakata-kafka-connect
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: datakata-connect
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components
    volumes:
      - ./docker/minio/seed-data:/data/spooldir/input
      - spooldir-finished:/data/spooldir/finished
      - spooldir-error:/data/spooldir/error
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8083/connectors || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s
    networks:
      - datakata

  # ─── SOAP WS-* Service ────────────────────────────────────────────
  soap-service:
    build:
      context: ./soap-service
      dockerfile: Dockerfile
    container_name: datakata-soap-service
    ports:
      - "8090:8090"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8090/ws/sales?wsdl || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - datakata

  # ─── WS Producer (SOAP → Kafka) ───────────────────────────────────
  ws-producer:
    build:
      context: ./ws-producer
      dockerfile: Dockerfile
    container_name: datakata-ws-producer
    depends_on:
      kafka:
        condition: service_healthy
      soap-service:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      SOAP_WSDL_URL: http://soap-service:8090/ws/sales?wsdl
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8091/actuator/health || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - datakata

  # ─── ClickHouse (Results DB) ───────────────────────────────────────
  clickhouse:
    image: clickhouse/clickhouse-server:24.3
    container_name: datakata-clickhouse
    ports:
      - "8123:8123"
      - "9010:9000"
    volumes:
      - clickhouse-data:/var/lib/clickhouse
      - ./docker/clickhouse/init.sql:/docker-entrypoint-initdb.d/init.sql
    environment:
      CLICKHOUSE_DB: datakata
      CLICKHOUSE_USER: datakata
      CLICKHOUSE_PASSWORD: datakata
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    healthcheck:
      test: ["CMD-SHELL", "clickhouse-client --user datakata --password datakata --query 'SELECT 1'"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 15s
    networks:
      - datakata

  # ─── Flink JobManager ─────────────────────────────────────────────
  flink-jobmanager:
    build:
      context: ./processing
      dockerfile: Dockerfile
    container_name: datakata-flink-jobmanager
    command: jobmanager
    ports:
      - "8081:8081"
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      CLICKHOUSE_URL: "jdbc:ch://clickhouse:8123/datakata"
      CLICKHOUSE_USER: datakata
      CLICKHOUSE_PASSWORD: datakata
      MARQUEZ_URL: "http://marquez:5000/api/v1/lineage"
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        jobmanager.rpc.port: 6123
        jobmanager.memory.process.size: 1600m
        rest.flamegraph.enabled: true
        metrics.reporters: prom
        metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
        metrics.reporter.prom.port: 9249
    depends_on:
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081/overview || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - datakata

  flink-taskmanager:
    build:
      context: ./processing
      dockerfile: Dockerfile
    command: taskmanager
    deploy:
      replicas: 2
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      CLICKHOUSE_URL: "jdbc:ch://clickhouse:8123/datakata"
      CLICKHOUSE_USER: datakata
      CLICKHOUSE_PASSWORD: datakata
      MARQUEZ_URL: "http://marquez:5000/api/v1/lineage"
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        jobmanager.rpc.port: 6123
        taskmanager.numberOfTaskSlots: 4
        taskmanager.memory.process.size: 2048m
        metrics.reporters: prom
        metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
        metrics.reporter.prom.port: 9249
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    networks:
      - datakata

  # ─── Results API (Kotlin + Spring Boot) ────────────────────────────
  results-api:
    build:
      context: ./results-api
      dockerfile: Dockerfile
    container_name: datakata-results-api
    depends_on:
      clickhouse:
        condition: service_healthy
      kafka:
        condition: service_healthy
    ports:
      - "8080:8080"
    environment:
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_DATABASE: datakata
      CLICKHOUSE_USER: datakata
      CLICKHOUSE_PASSWORD: datakata
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/api/v1/health || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 20s
    networks:
      - datakata

  # ─── Marquez (Data Lineage) ────────────────────────────────────────
  marquez-db:
    image: postgres:16
    container_name: datakata-marquez-db
    environment:
      POSTGRES_DB: marquez
      POSTGRES_USER: marquez
      POSTGRES_PASSWORD: marquez
    volumes:
      - marquez-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U marquez -d marquez"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - datakata

  marquez:
    image: marquezproject/marquez:0.47.0
    container_name: datakata-marquez
    depends_on:
      marquez-db:
        condition: service_healthy
    ports:
      - "5000:5000"
      - "5001:5001"
    environment:
      MARQUEZ_PORT: 5000
      MARQUEZ_ADMIN_PORT: 5001
      POSTGRES_HOST: marquez-db
      POSTGRES_PORT: 5432
      POSTGRES_DB: marquez
      POSTGRES_USER: marquez
      POSTGRES_PASSWORD: marquez
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5000/api/v1/namespaces || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - datakata

  marquez-web:
    image: marquezproject/marquez-web:0.47.0
    container_name: datakata-marquez-web
    depends_on:
      marquez:
        condition: service_healthy
    ports:
      - "3001:3000"
    environment:
      MARQUEZ_HOST: marquez
      MARQUEZ_PORT: 5000
    networks:
      - datakata

  # ─── Prometheus ────────────────────────────────────────────────────
  prometheus:
    image: prom/prometheus:v2.51.0
    container_name: datakata-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./observability/alerts/rules.yml:/etc/prometheus/rules.yml
      - prometheus-data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-lifecycle"
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - datakata

  # ─── Grafana ───────────────────────────────────────────────────────
  grafana:
    image: grafana/grafana:10.4.1
    container_name: datakata-grafana
    depends_on:
      prometheus:
        condition: service_healthy
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: "false"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./docker/grafana/provisioning:/etc/grafana/provisioning
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - datakata
